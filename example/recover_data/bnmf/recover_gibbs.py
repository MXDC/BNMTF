"""
Recover the toy dataset generated by example/generate_toy/bnmf/generate_bnmf.py
We use the parameters for the true priors.

We can plot the values Uik, Vjk, or tau, to see how the Gibbs sampler converges. 
"""

project_location = "/home/tab43/Documents/Projects/libraries/"
import sys
sys.path.append(project_location)

from BNMTF.code.bnmf_gibbs import bnmf_gibbs
from ml_helpers.code.mask import calc_inverse_M

import numpy, matplotlib.pyplot as plt

##########

input_folder = project_location+"BNMTF/example/generate_toy/bnmf/"

iterations = 100
init = 'random'
I, J, K = 20, 10, 3 #100, 50, 10

alpha, beta = 10., 1.
lambdaU = numpy.ones((I,K))
lambdaV = numpy.ones((I,K))/2.    
priors = { 'alpha':alpha, 'beta':beta, 'lambdaU':lambdaU, 'lambdaV':lambdaV }

# Load in data
R = numpy.loadtxt(input_folder+"R.txt")
M = numpy.loadtxt(input_folder+"M.txt")
M_test = calc_inverse_M(M)

# Run the Gibbs sampler
BNMF = bnmf_gibbs(R,M,K,priors)
BNMF.initialise(init)
BNMF.run(iterations)

taus = BNMF.all_tau
Us = BNMF.all_U
Vs = BNMF.all_V

# Plot tau against iterations to see that it converges
def subplot(plotval,Y,name_plot,name_y):
    plt.subplot(plotval)
    plt.plot(range(1,len(Y)+1),Y)
    plt.xlabel("Iterations")
    plt.ylabel(name_y)
    
plt.figure(1)
subplot(211,taus,'Tau against iterations','tau')
subplot(212,Us[:,0,0],'U[0,0] against iterations','U[0,0]')
plt.show()

# Approximate the expectations
burn_in = 10
thinning = 2
(exp_U, exp_V, exp_tau) = BNMF.approx_expectation(burn_in,thinning)

# Also measure the performances
performances = BNMF.predict(M_test,burn_in,thinning)
print performances